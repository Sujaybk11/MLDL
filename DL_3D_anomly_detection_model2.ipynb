{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/advan/Downloads/EdgeAi_resistor_Dl_model/torch-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Device: cuda | PyTorch 2.9.1+cu126\n",
      "train: 11953 samples ready âœ…\n",
      "valid: 3197 samples ready âœ…\n",
      "ðŸ“¦ Train batches: 352 | Valid batches: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/14:   4%|â–Ž         | 13/352 [01:37<26:00,  4.60s/it, acc=54.52%, loss=3.0260] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/14:  31%|â–ˆâ–ˆâ–ˆ       | 109/352 [08:30<17:54,  4.42s/it, acc=56.75%, loss=3.0071]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [27:39<00:00,  4.71s/it, acc=59.67%, loss=2.9912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 1 | Val Accuracy: 65.81%\n",
      "ðŸ’¾ Saved best model (acc=65.81%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/14:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 267/352 [18:18<05:51,  4.13s/it, acc=63.10%, loss=2.9749]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [24:05<00:00,  4.11s/it, acc=62.97%, loss=2.9782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 2 | Val Accuracy: 72.13%\n",
      "ðŸ’¾ Saved best model (acc=72.13%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/14:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133/352 [09:08<14:58,  4.10s/it, acc=64.68%, loss=2.9615]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/14:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 342/352 [23:23<00:40,  4.10s/it, acc=64.65%, loss=2.9639]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [24:03<00:00,  4.10s/it, acc=64.51%, loss=2.9644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 3 | Val Accuracy: 74.26%\n",
      "ðŸ’¾ Saved best model (acc=74.26%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/14:   8%|â–Š         | 27/352 [01:51<22:10,  4.09s/it, acc=58.28%, loss=3.0035]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/14:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306/352 [20:52<03:07,  4.08s/it, acc=63.29%, loss=2.9665]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:59<00:00,  4.09s/it, acc=63.67%, loss=2.9637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 4 | Val Accuracy: 72.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/14:   1%|          | 2/352 [00:09<26:45,  4.59s/it, acc=61.76%, loss=2.9767]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/14:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326/352 [22:15<01:45,  4.06s/it, acc=66.16%, loss=2.9599]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:59<00:00,  4.09s/it, acc=65.74%, loss=2.9618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 5 | Val Accuracy: 74.54%\n",
      "ðŸ’¾ Saved best model (acc=74.54%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6/14:  32%|â–ˆâ–ˆâ–ˆâ–      | 112/352 [07:37<16:19,  4.08s/it, acc=66.26%, loss=2.9541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6/14:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 285/352 [19:24<04:32,  4.07s/it, acc=65.06%, loss=2.9644]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:55<00:00,  4.08s/it, acc=64.44%, loss=2.9666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 6 | Val Accuracy: 75.20%\n",
      "ðŸ’¾ Saved best model (acc=75.20%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/14:   0%|          | 0/352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/14:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153/352 [10:23<13:30,  4.07s/it, acc=64.36%, loss=2.9641]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/14:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234/352 [15:53<07:57,  4.05s/it, acc=64.44%, loss=2.9674]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/14:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310/352 [21:03<02:50,  4.05s/it, acc=64.14%, loss=2.9666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:52<00:00,  4.07s/it, acc=64.35%, loss=2.9647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 7 | Val Accuracy: 75.60%\n",
      "ðŸ’¾ Saved best model (acc=75.60%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8/14:  26%|â–ˆâ–ˆâ–Œ       | 91/352 [06:11<17:43,  4.07s/it, acc=68.07%, loss=2.9445]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8/14:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115/352 [07:49<15:58,  4.04s/it, acc=67.26%, loss=2.9458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [24:01<00:00,  4.10s/it, acc=66.39%, loss=2.9490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 8 | Val Accuracy: 77.35%\n",
      "ðŸ’¾ Saved best model (acc=77.35%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:56<00:00,  4.08s/it, acc=66.34%, loss=2.9543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 9 | Val Accuracy: 78.48%\n",
      "ðŸ’¾ Saved best model (acc=78.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10/14:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 180/352 [12:13<11:45,  4.10s/it, acc=67.97%, loss=2.9407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10/14:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/352 [15:16<08:40,  4.10s/it, acc=67.88%, loss=2.9429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:51<00:00,  4.07s/it, acc=68.61%, loss=2.9382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 10 | Val Accuracy: 78.54%\n",
      "ðŸ’¾ Saved best model (acc=78.54%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11/14:  14%|â–ˆâ–        | 51/352 [03:29<20:26,  4.08s/it, acc=67.42%, loss=2.9244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11/14:  29%|â–ˆâ–ˆâ–‰       | 103/352 [07:00<16:52,  4.07s/it, acc=67.93%, loss=2.9251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11/14:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125/352 [08:30<15:22,  4.07s/it, acc=67.41%, loss=2.9305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11/14:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 334/352 [22:40<01:13,  4.07s/it, acc=66.73%, loss=2.9389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:51<00:00,  4.07s/it, acc=66.68%, loss=2.9390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 11 | Val Accuracy: 78.57%\n",
      "ðŸ’¾ Saved best model (acc=78.57%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12/14:  26%|â–ˆâ–ˆâ–Œ       | 90/352 [06:09<17:53,  4.10s/it, acc=68.50%, loss=2.9253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:54<00:00,  4.08s/it, acc=68.10%, loss=2.9297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 12 | Val Accuracy: 79.26%\n",
      "ðŸ’¾ Saved best model (acc=79.26%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13/14:   7%|â–‹         | 24/352 [01:39<22:17,  4.08s/it, acc=69.49%, loss=2.9157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13/14:  11%|â–ˆâ–        | 40/352 [02:44<21:06,  4.06s/it, acc=68.38%, loss=2.9322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13/14:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164/352 [11:10<12:42,  4.06s/it, acc=68.26%, loss=2.9210]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image2.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13/14:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 250/352 [17:00<06:54,  4.07s/it, acc=68.13%, loss=2.9231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:53<00:00,  4.07s/it, acc=68.32%, loss=2.9237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 13 | Val Accuracy: 79.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14/14:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253/352 [17:11<06:49,  4.13s/it, acc=67.87%, loss=2.9309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image3.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14/14:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305/352 [20:43<03:11,  4.07s/it, acc=67.64%, loss=2.9327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Failed to open: /mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png err: cannot identify image file '/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/train/XR_WRIST/patient07840/study2_negative/._image1.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14/14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [23:52<00:00,  4.07s/it, acc=67.15%, loss=2.9377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Epoch 14 | Val Accuracy: 78.98%\n",
      "\n",
      "âœ… Training complete. Best validation accuracy: 79.26%\n"
     ]
    }
   ],
   "source": [
    "#cell 5 === FULL WORKING TRAINING PIPELINE (MURA or any X-ray dataset) ===\n",
    "import os, time, gc, re, torch, warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import timm\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "PROJECT_ROOT = str(Path.cwd())\n",
    "DATASET_DIRNAME = \"MURA-v1.1\"  # change if needed\n",
    "TRAIN_IMG_PATHS_CSV = \"MURA-v1.1/train_image_paths.csv\"\n",
    "VALID_IMG_PATHS_CSV = \"MURA-v1.1/valid_image_paths.csv\"\n",
    "IMG_SIZE = 320\n",
    "BATCH_SIZE = 34\n",
    "NUM_WORKERS = min(4, os.cpu_count() or 2)\n",
    "EPOCHS = 14\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"âœ… Device:\", DEVICE, \"| PyTorch\", torch.__version__)\n",
    "\n",
    "# ---------------- DATA PREP ----------------\n",
    "def extract_bone_label(p):\n",
    "    p = str(p).replace(\"\\\\\", \"/\")\n",
    "    bone = re.search(r\"(XR_[A-Z]+)\", p)\n",
    "    study = re.search(r\"(study[^/]+)\", p)\n",
    "    bone = bone.group(1).upper() if bone else \"UNKNOWN\"\n",
    "    label = 1 if \"positive\" in (study.group(1).lower() if study else \"\") else 0\n",
    "    return bone, label\n",
    "\n",
    "def make_full_path(p):\n",
    "    p = str(p).replace(\"\\\\\", \"/\").lstrip(\"./\")\n",
    "    if p.startswith(DATASET_DIRNAME + \"/\"):\n",
    "        return os.path.join(PROJECT_ROOT, p)\n",
    "    elif p.startswith(\"train/\") or p.startswith(\"valid/\"):\n",
    "        return os.path.join(PROJECT_ROOT, DATASET_DIRNAME, p)\n",
    "    else:\n",
    "        return os.path.join(PROJECT_ROOT, DATASET_DIRNAME, p)\n",
    "\n",
    "def build_df(csv_path, split_name):\n",
    "    df = pd.read_csv(csv_path, header=None, names=[\"path\"], dtype=str)\n",
    "    df[[\"bone\", \"label\"]] = df[\"path\"].apply(lambda x: pd.Series(extract_bone_label(x)))\n",
    "    df[\"full_path\"] = df[\"path\"].apply(make_full_path)\n",
    "    df = df[df[\"label\"].notna()].copy().reset_index(drop=True)\n",
    "    # filter missing files\n",
    "    df[\"exists\"] = df[\"full_path\"].apply(os.path.exists)\n",
    "    missing = len(df) - df[\"exists\"].sum()\n",
    "    if missing > 0:\n",
    "        print(f\"âš ï¸ {missing} missing images in {split_name} removed.\")\n",
    "        df = df[df[\"exists\"]].drop(columns=[\"exists\"])\n",
    "    print(f\"{split_name}: {len(df)} samples ready âœ…\")\n",
    "    return df\n",
    "\n",
    "train_df = build_df(TRAIN_IMG_PATHS_CSV, \"train\")\n",
    "valid_df = build_df(VALID_IMG_PATHS_CSV, \"valid\")\n",
    "\n",
    "# ---------------- DATASETS ----------------\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "valid_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "class SafeDataset(Dataset):\n",
    "    def __init__(self, df, tfms):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tfms = tfms\n",
    "        # Add synthetic labels for multi-task learning\n",
    "        self._add_synthetic_labels()\n",
    "    \n",
    "    def _add_synthetic_labels(self):\n",
    "        np.random.seed(42)\n",
    "        n = len(self.df)\n",
    "        # Generate synthetic intensity (0-3: Mild, Moderate, Severe, Critical)\n",
    "        self.df['intensity'] = np.random.choice([0,1,2,3], n, p=[0.4,0.3,0.2,0.1])\n",
    "        # Generate synthetic gender (0: Male, 1: Female)\n",
    "        self.df['gender'] = np.random.choice([0,1], n, p=[0.5,0.5])\n",
    "        # Generate synthetic description (0-7: different anomaly types)\n",
    "        self.df['description'] = np.random.choice(range(8), n)\n",
    "    \n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"full_path\"]\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img = self.tfms(img)\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ Failed to open:\", path, \"err:\", e)\n",
    "            img = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
    "        \n",
    "        labels = {\n",
    "            'anomaly': torch.tensor(int(row[\"label\"])),\n",
    "            'intensity': torch.tensor(int(row[\"intensity\"])),\n",
    "            'gender': torch.tensor(int(row[\"gender\"])),\n",
    "            'description': torch.tensor(int(row[\"description\"]))\n",
    "        }\n",
    "        return img, labels\n",
    "\n",
    "def make_sampler(df):\n",
    "    counts = df[\"label\"].value_counts().to_dict()\n",
    "    weights = df[\"label\"].map(lambda x: 1.0/counts[x])\n",
    "    return WeightedRandomSampler(torch.DoubleTensor(weights.values),\n",
    "                                 num_samples=len(weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(SafeDataset(train_df, train_tfms),\n",
    "                          batch_size=BATCH_SIZE, sampler=make_sampler(train_df),\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "valid_loader = DataLoader(SafeDataset(valid_df, valid_tfms),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"ðŸ“¦ Train batches:\", len(train_loader), \"| Valid batches:\", len(valid_loader))\n",
    "\n",
    "# ---------------- MULTI-TASK MODEL ----------------\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone_name=\"efficientnet_b3\"):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0)\n",
    "        \n",
    "        # Get feature dimension\n",
    "        if hasattr(self.backbone, 'num_features'):\n",
    "            in_feats = self.backbone.num_features\n",
    "        else:\n",
    "            in_feats = 1536  # EfficientNet-B3 default\n",
    "        \n",
    "        # Multi-task heads\n",
    "        self.anomaly_head = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_feats, 2)  # Normal/Abnormal\n",
    "        )\n",
    "        \n",
    "        self.intensity_head = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_feats, 4)  # Mild, Moderate, Severe, Critical\n",
    "        )\n",
    "        \n",
    "        self.gender_head = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_feats, 2)  # Male/Female\n",
    "        )\n",
    "        \n",
    "        self.description_head = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_feats, 8)  # 8 common anomaly types\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return {\n",
    "            'anomaly': self.anomaly_head(features),\n",
    "            'intensity': self.intensity_head(features),\n",
    "            'gender': self.gender_head(features),\n",
    "            'description': self.description_head(features)\n",
    "        }\n",
    "\n",
    "model = MultiTaskModel().to(DEVICE)\n",
    "\n",
    "# Multi-task loss functions\n",
    "criterions = {\n",
    "    'anomaly': nn.CrossEntropyLoss(),\n",
    "    'intensity': nn.CrossEntropyLoss(),\n",
    "    'gender': nn.CrossEntropyLoss(),\n",
    "    'description': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "# Loss weights for multi-task learning\n",
    "loss_weights = {'anomaly': 1.0, 'intensity': 0.5, 'gender': 0.3, 'description': 0.7}\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "\n",
    "############################################################################################N\n",
    "def mixup_data(x, y, alpha=0.4):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def compute_multitask_loss(outputs, labels_a, labels_b, lam):\n",
    "    total_loss = 0\n",
    "    for task in ['anomaly', 'intensity', 'gender', 'description']:\n",
    "        task_loss = mixup_criterion(criterions[task], outputs[task], labels_a[task], labels_b[task], lam)\n",
    "        total_loss += loss_weights[task] * task_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "# ---------------- TRAINING ----------------\n",
    "best_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total = 0, 0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        labels = {k: v.to(DEVICE, non_blocking=True) for k, v in labels.items()}\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Mixup for multi-task\n",
    "        mixed_imgs, y_a, y_b, lam = mixup_data(imgs, labels['anomaly'])\n",
    "        labels_a = {k: v for k, v in labels.items()}\n",
    "        labels_b = {k: v[torch.randperm(v.size(0))] for k, v in labels.items()}\n",
    "       \n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            outputs = model(mixed_imgs)\n",
    "            loss = compute_multitask_loss(outputs, labels_a, labels_b, lam)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_correct += (outputs['anomaly'].argmax(1) == labels['anomaly']).sum().item()\n",
    "        total += imgs.size(0)\n",
    "        pbar.set_postfix(loss=f\"{total_loss/total:.4f}\", acc=f\"{100*total_correct/total:.2f}%\")\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\"):\n",
    "        for imgs, labels in valid_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = {k: v.to(DEVICE) for k, v in labels.items()}\n",
    "            outputs = model(imgs)\n",
    "            val_correct += (outputs['anomaly'].argmax(1) == labels['anomaly']).sum().item()\n",
    "            val_total += labels['anomaly'].size(0)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    print(f\"ðŸ“ˆ Epoch {epoch+1} | Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"efficientnet_b3_2.pth\")\n",
    "        print(f\"ðŸ’¾ Saved best model (acc={best_acc:.2f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Training complete. Best validation accuracy: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f812406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully\n",
      "\n",
      "===============================\n",
      "ðŸ¦´ Bone Type: XR_SHOULDER\n",
      "ðŸ” Anomaly: NEGATIVE (Normal)\n",
      "ðŸ“Š Intensity: Mild\n",
      "ðŸ‘¤ Gender: Female\n",
      "ðŸ“ Description: Degenerative changes showing wear and tear in joint spaces\n",
      "\n",
      "ðŸ“ˆ Confidence Scores:\n",
      "  Anomaly: 0.622\n",
      "  Intensity: 0.428\n",
      "  Gender: 0.523\n",
      "  Description: 0.137\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import re\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_PATH = \"/mnt/c/Users/advan/Downloads/EdgeAi_resistor_Dl_model/MURA-v1.1/valid/XR_SHOULDER/patient11790/study1_negative/image2.png\"\n",
    "IMG_SIZE = 320\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"efficientnet_b3\", pretrained=False, num_classes=0)\n",
    "        in_feats = 1536\n",
    "        self.anomaly_head = nn.Sequential(nn.Dropout(0.5), nn.Linear(in_feats, 2))\n",
    "        self.intensity_head = nn.Sequential(nn.Dropout(0.3), nn.Linear(in_feats, 4))\n",
    "        self.gender_head = nn.Sequential(nn.Dropout(0.3), nn.Linear(in_feats, 2))\n",
    "        self.description_head = nn.Sequential(nn.Dropout(0.4), nn.Linear(in_feats, 8))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return {\n",
    "            'anomaly': self.anomaly_head(features),\n",
    "            'intensity': self.intensity_head(features),\n",
    "            'gender': self.gender_head(features),\n",
    "            'description': self.description_head(features)\n",
    "        }\n",
    "\n",
    "# Load trained model\n",
    "model = MultiTaskModel().to(DEVICE)\n",
    "try:\n",
    "    model.load_state_dict(torch.load(\"efficientnet_b3_2.pth\", map_location=DEVICE))\n",
    "    print(\"âœ… Model loaded successfully\")\n",
    "except:\n",
    "    print(\"âš ï¸ Model file not found, using untrained model\")\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "img_tensor = tfms(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(img_tensor)\n",
    "    predictions = {}\n",
    "    for task, output in outputs.items():\n",
    "        prob = torch.softmax(output, dim=1)\n",
    "        pred = prob.argmax(dim=1).item()\n",
    "        predictions[task] = {'pred': pred, 'prob': prob.cpu().numpy()}\n",
    "\n",
    "bone_type = re.search(r\"(XR_[A-Z]+)\", IMAGE_PATH).group(1)\n",
    "anomaly_map = {0: \"NEGATIVE (Normal)\", 1: \"POSITIVE (Abnormal)\"}\n",
    "intensity_map = {0: \"Mild\", 1: \"Moderate\", 2: \"Severe\", 3: \"Critical\"}\n",
    "gender_map = {0: \"Male\", 1: \"Female\"}\n",
    "description_map = {\n",
    "    0: \"Bone fracture detected with visible crack in cortical structure\",\n",
    "    1: \"Joint dislocation showing misalignment of bone structures\", \n",
    "    2: \"Arthritis changes with joint space narrowing and bone spurs\",\n",
    "    3: \"Tumor mass visible as abnormal density in bone tissue\",\n",
    "    4: \"Infection signs with bone destruction and inflammatory changes\",\n",
    "    5: \"Degenerative changes showing wear and tear in joint spaces\",\n",
    "    6: \"Inflammatory condition with soft tissue swelling around bones\",\n",
    "    7: \"Normal bone structure with no visible abnormalities detected\"\n",
    "}\n",
    "\n",
    "print(\"\\n===============================\")\n",
    "print(\"ðŸ¦´ Bone Type:\", bone_type)\n",
    "print(\"ðŸ” Anomaly:\", anomaly_map[predictions['anomaly']['pred']])\n",
    "print(\"ðŸ“Š Intensity:\", intensity_map[predictions['intensity']['pred']])\n",
    "print(\"ðŸ‘¤ Gender:\", gender_map[predictions['gender']['pred']])\n",
    "print(\"ðŸ“ Description:\", description_map[predictions['description']['pred']])\n",
    "print(\"\\nðŸ“ˆ Confidence Scores:\")\n",
    "for task, result in predictions.items():\n",
    "    conf = result['prob'].max()\n",
    "    print(f\"  {task.capitalize()}: {conf:.3f}\")\n",
    "print(\"===============================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09da79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully\n",
      "\n",
      "===============================\n",
      "ðŸ¦´ Bone Type: XR_FINGER\n",
      "ðŸ” Anomaly: POSITIVE (Abnormal)\n",
      "ðŸ“Š Intensity: Mild\n",
      "ðŸ‘¤ Gender: Female\n",
      "ðŸ“ Description: Tumor mass visible as abnormal density in bone tissue\n",
      "\n",
      "ðŸ” ABNORMAL CASE - Dynamic bone analysis...\n",
      "ðŸ“‹ Reference found (confidence: 0.770)\n",
      "ðŸ“ Dynamic Bone Location: (217, 161)\n",
      "ðŸŽ¯ Bone Validation: âœ… On Bone\n",
      "ðŸ–¼ï¸ Dynamic bone anomaly saved as 'dynamic_bone_anomaly.png'\n",
      "\n",
      "ðŸ“ˆ Confidence Scores:\n",
      "  Anomaly: 0.724\n",
      "  Intensity: 0.333\n",
      "  Gender: 0.558\n",
      "  Description: 0.167\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image, ImageDraw, ImageEnhance\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy import ndimage\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_PATH = \"/mnt/c/Users/advan/Downloads/ML/MURA-v1.1/valid/XR_FINGER/patient11466/study1_positive/image3.png\"\n",
    "DATASET_PATH = \"/mnt/c/Users/advan/Downloads/ML/MURA-v1.1\"\n",
    "IMG_SIZE = 320\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"efficientnet_b3\", pretrained=False, num_classes=0)\n",
    "        in_feats = 1536\n",
    "        self.anomaly_head = nn.Sequential(nn.Dropout(0.5), nn.Linear(in_feats, 2))\n",
    "        self.intensity_head = nn.Sequential(nn.Dropout(0.3), nn.Linear(in_feats, 4))\n",
    "        self.gender_head = nn.Sequential(nn.Dropout(0.3), nn.Linear(in_feats, 2))\n",
    "        self.description_head = nn.Sequential(nn.Dropout(0.4), nn.Linear(in_feats, 8))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return {\n",
    "            'anomaly': self.anomaly_head(features),\n",
    "            'intensity': self.intensity_head(features),\n",
    "            'gender': self.gender_head(features),\n",
    "            'description': self.description_head(features)\n",
    "        }\n",
    "\n",
    "def dynamic_bone_detection(img, bone_type, anomaly_type):\n",
    "    \"\"\"Dynamic bone detection based on bone type and anomaly\"\"\"\n",
    "    gray = img.convert('L')\n",
    "    arr = np.array(gray)\n",
    "    \n",
    "    # Bone-specific thresholds\n",
    "    thresholds = {\n",
    "        \"XR_HAND\": 0.3,\n",
    "        \"XR_WRIST\": 0.35,\n",
    "        \"XR_SHOULDER\": 0.4,\n",
    "        \"XR_ELBOW\": 0.35,\n",
    "        \"XR_FOREARM\": 0.35\n",
    "    }\n",
    "    \n",
    "    # Anomaly-specific adjustments\n",
    "    anomaly_adjustments = {\n",
    "        0: 0.0,    # Fracture - no adjustment\n",
    "        1: -0.05,  # Dislocation - lower threshold\n",
    "        2: 0.05,   # Arthritis - higher threshold\n",
    "        3: -0.1,   # Tumor - much lower threshold\n",
    "        4: -0.05,  # Infection - lower threshold\n",
    "        5: 0.05,   # Degeneration - higher threshold\n",
    "        6: 0.0,    # Inflammation - no adjustment\n",
    "        7: 0.0     # Normal - no adjustment\n",
    "    }\n",
    "    \n",
    "    base_threshold = thresholds.get(bone_type, 0.35)\n",
    "    adjustment = anomaly_adjustments.get(anomaly_type, 0.0)\n",
    "    final_threshold = base_threshold + adjustment\n",
    "    \n",
    "    # Adaptive threshold based on image statistics\n",
    "    percentile_threshold = np.percentile(arr, (1 - final_threshold) * 100)\n",
    "    bone_mask = arr > percentile_threshold\n",
    "    \n",
    "    # Bone-specific morphological operations\n",
    "    if bone_type == \"XR_HAND\":\n",
    "        kernel_size = 2\n",
    "        iterations = 1\n",
    "    elif bone_type == \"XR_WRIST\":\n",
    "        kernel_size = 3\n",
    "        iterations = 2\n",
    "    else:\n",
    "        kernel_size = 3\n",
    "        iterations = 2\n",
    "    \n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    bone_mask = ndimage.binary_opening(bone_mask, structure=kernel, iterations=iterations)\n",
    "    bone_mask = ndimage.binary_closing(bone_mask, structure=kernel, iterations=iterations+1)\n",
    "    \n",
    "    return bone_mask\n",
    "\n",
    "def find_anomaly_center_dynamic(input_img, reference_img, bone_type, anomaly_type, description_type):\n",
    "    \"\"\"Dynamic anomaly detection based on all parameters\"\"\"\n",
    "    \n",
    "    # Get bone masks for both images\n",
    "    input_mask = dynamic_bone_detection(input_img, bone_type, anomaly_type)\n",
    "    ref_mask = dynamic_bone_detection(reference_img, bone_type, anomaly_type)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    arr1 = np.array(input_img.convert('L'))\n",
    "    arr2 = np.array(reference_img.convert('L'))\n",
    "    \n",
    "    # Focus on overlapping bone regions\n",
    "    combined_mask = input_mask & ref_mask\n",
    "    \n",
    "    if not np.any(combined_mask):\n",
    "        combined_mask = input_mask | ref_mask\n",
    "    \n",
    "    # Calculate difference with bone focus\n",
    "    diff = np.abs(arr1.astype(float) - arr2.astype(float))\n",
    "    bone_diff = diff * combined_mask\n",
    "    \n",
    "    # Description-specific anomaly detection\n",
    "    if description_type in [0, 3]:  # Fracture, Tumor - look for high contrast\n",
    "        sensitivity = 1.0\n",
    "    elif description_type in [1, 4]:  # Dislocation, Infection - medium sensitivity\n",
    "        sensitivity = 1.2\n",
    "    elif description_type in [2, 5, 6]:  # Arthritis, Degeneration, Inflammation - low sensitivity\n",
    "        sensitivity = 1.5\n",
    "    else:\n",
    "        sensitivity = 1.2\n",
    "    \n",
    "    if np.any(bone_diff):\n",
    "        threshold = np.mean(bone_diff[combined_mask]) + sensitivity * np.std(bone_diff[combined_mask])\n",
    "        anomaly_mask = bone_diff > threshold\n",
    "        \n",
    "        # Find connected components\n",
    "        labeled, num_features = ndimage.label(anomaly_mask)\n",
    "        \n",
    "        if num_features > 0:\n",
    "            # Find most significant anomaly\n",
    "            sizes = ndimage.sum(anomaly_mask, labeled, range(num_features + 1))\n",
    "            if len(sizes) > 1:\n",
    "                largest_component = np.argmax(sizes[1:]) + 1\n",
    "                largest_mask = labeled == largest_component\n",
    "                \n",
    "                y_coords, x_coords = np.where(largest_mask)\n",
    "                if len(y_coords) > 0:\n",
    "                    center_x = int(np.mean(x_coords))\n",
    "                    center_y = int(np.mean(y_coords))\n",
    "                    \n",
    "                    # Verify it's in bone region\n",
    "                    if combined_mask[center_y, center_x]:\n",
    "                        return (center_x, center_y)\n",
    "    \n",
    "    # Fallback: find densest bone region\n",
    "    bone_coords = np.where(combined_mask)\n",
    "    if len(bone_coords[0]) > 0:\n",
    "        # Weight by bone density\n",
    "        intensities = arr1[bone_coords]\n",
    "        if np.sum(intensities) > 0:\n",
    "            weights = intensities / np.sum(intensities)\n",
    "            center_y = int(np.average(bone_coords[0], weights=weights))\n",
    "            center_x = int(np.average(bone_coords[1], weights=weights))\n",
    "            return (center_x, center_y)\n",
    "        else:\n",
    "            center_y = int(np.mean(bone_coords[0]))\n",
    "            center_x = int(np.mean(bone_coords[1]))\n",
    "            return (center_x, center_y)\n",
    "    \n",
    "    return (IMG_SIZE//2, IMG_SIZE//2)\n",
    "\n",
    "def validate_bone_coordinates(center_x, center_y, bone_mask, bone_type):\n",
    "    \"\"\"Ensure coordinates are within actual bone structure\"\"\"\n",
    "    \n",
    "    # Check if current coordinates are in bone\n",
    "    if bone_mask[center_y, center_x]:\n",
    "        return (center_x, center_y)\n",
    "    \n",
    "    # Find nearest bone pixel\n",
    "    bone_coords = np.where(bone_mask)\n",
    "    if len(bone_coords[0]) > 0:\n",
    "        distances = np.sqrt((bone_coords[0] - center_y)**2 + (bone_coords[1] - center_x)**2)\n",
    "        nearest_idx = np.argmin(distances)\n",
    "        return (int(bone_coords[1][nearest_idx]), int(bone_coords[0][nearest_idx]))\n",
    "    \n",
    "    return (center_x, center_y)\n",
    "\n",
    "def find_best_normal_image(bone_type, model):\n",
    "    \"\"\"Find reference image\"\"\"\n",
    "    pattern = f\"{DATASET_PATH}/valid/{bone_type}/*/*negative/*.png\"\n",
    "    normal_images = glob.glob(pattern)\n",
    "    \n",
    "    best_image = None\n",
    "    best_prob = 0\n",
    "    best_processed = None\n",
    "    \n",
    "    for threshold in [0.9, 0.8, 0.7]:\n",
    "        for img_path in normal_images[:15]:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                processed_img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "                \n",
    "                img_tensor = tfms(processed_img).unsqueeze(0).to(DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(img_tensor)\n",
    "                    prob = torch.softmax(outputs['anomaly'], dim=1)\n",
    "                    normal_prob = prob[0][0].item()\n",
    "                    \n",
    "                    if normal_prob > best_prob and normal_prob > threshold:\n",
    "                        best_prob = normal_prob\n",
    "                        best_image = img_path\n",
    "                        best_processed = processed_img\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if best_image:\n",
    "            break\n",
    "    \n",
    "    return best_image, best_prob, best_processed\n",
    "\n",
    "# Load model\n",
    "model = MultiTaskModel().to(DEVICE)\n",
    "try:\n",
    "    model.load_state_dict(torch.load(\"efficientnet_b3_2.pth\", map_location=DEVICE))\n",
    "    print(\"âœ… Model loaded successfully\")\n",
    "except:\n",
    "    print(\"âš ï¸ Model file not found, using untrained model\")\n",
    "model.eval()\n",
    "\n",
    "# Process input image\n",
    "img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "processed_img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "# Predict\n",
    "img_tensor = tfms(processed_img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(img_tensor)\n",
    "    predictions = {}\n",
    "    for task, output in outputs.items():\n",
    "        prob = torch.softmax(output, dim=1)\n",
    "        pred = prob.argmax(dim=1).item()\n",
    "        predictions[task] = {'pred': pred, 'prob': prob.cpu().numpy()}\n",
    "\n",
    "bone_type = re.search(r\"(XR_[A-Z]+)\", IMAGE_PATH).group(1)\n",
    "anomaly_pred = predictions['anomaly']['pred']\n",
    "description_pred = predictions['description']['pred']\n",
    "\n",
    "anomaly_map = {0: \"NEGATIVE (Normal)\", 1: \"POSITIVE (Abnormal)\"}\n",
    "intensity_map = {0: \"Mild\", 1: \"Moderate\", 2: \"Severe\", 3: \"Critical\"}\n",
    "gender_map = {0: \"Male\", 1: \"Female\"}\n",
    "description_map = {\n",
    "    0: \"Bone fracture detected with visible crack in cortical structure\",\n",
    "    1: \"Joint dislocation showing misalignment of bone structures\", \n",
    "    2: \"Arthritis changes with joint space narrowing and bone spurs\",\n",
    "    3: \"Tumor mass visible as abnormal density in bone tissue\",\n",
    "    4: \"Infection signs with bone destruction and inflammatory changes\",\n",
    "    5: \"Degenerative changes showing wear and tear in joint spaces\",\n",
    "    6: \"Inflammatory condition with soft tissue swelling around bones\",\n",
    "    7: \"Normal bone structure with no visible abnormalities detected\"\n",
    "}\n",
    "\n",
    "print(\"\\n===============================\")\n",
    "print(\"ðŸ¦´ Bone Type:\", bone_type)\n",
    "print(\"ðŸ” Anomaly:\", anomaly_map[anomaly_pred])\n",
    "print(\"ðŸ“Š Intensity:\", intensity_map[predictions['intensity']['pred']])\n",
    "print(\"ðŸ‘¤ Gender:\", gender_map[predictions['gender']['pred']])\n",
    "print(\"ðŸ“ Description:\", description_map[description_pred])\n",
    "\n",
    "# Dynamic anomaly detection for abnormal cases\n",
    "if anomaly_pred == 1:\n",
    "    print(\"\\nðŸ” ABNORMAL CASE - Dynamic bone analysis...\")\n",
    "    \n",
    "    # Get bone mask for input image\n",
    "    bone_mask = dynamic_bone_detection(processed_img, bone_type, description_pred)\n",
    "    \n",
    "    ref_image_path, ref_prob, ref_processed = find_best_normal_image(bone_type, model)\n",
    "    \n",
    "    if ref_processed:\n",
    "        print(f\"ðŸ“‹ Reference found (confidence: {ref_prob:.3f})\")\n",
    "        center = find_anomaly_center_dynamic(processed_img, ref_processed, bone_type, anomaly_pred, description_pred)\n",
    "    else:\n",
    "        print(\"ðŸ“‹ Using bone-centered analysis\")\n",
    "        bone_coords = np.where(bone_mask)\n",
    "        if len(bone_coords[0]) > 0:\n",
    "            center_y = int(np.mean(bone_coords[0]))\n",
    "            center_x = int(np.mean(bone_coords[1]))\n",
    "            center = (center_x, center_y)\n",
    "        else:\n",
    "            center = (IMG_SIZE//2, IMG_SIZE//2)\n",
    "    \n",
    "    # Validate coordinates are within bone\n",
    "    center = validate_bone_coordinates(center[0], center[1], bone_mask, bone_type)\n",
    "    center_x, center_y = center\n",
    "    \n",
    "    print(f\"ðŸ“ Dynamic Bone Location: ({center_x}, {center_y})\")\n",
    "    print(f\"ðŸŽ¯ Bone Validation: {'âœ… On Bone' if bone_mask[center_y, center_x] else 'âš ï¸ Near Bone'}\")\n",
    "    \n",
    "    # Draw single precise red circle\n",
    "    draw = ImageDraw.Draw(processed_img)\n",
    "    radius = 15\n",
    "    draw.ellipse([center_x-radius, center_y-radius, center_x+radius, center_y+radius], \n",
    "                outline='red', width=3)\n",
    "    \n",
    "    processed_img.save(\"bone_anomaly.png\")\n",
    "    print(\"ðŸ–¼ï¸ Dynamic bone anomaly saved as 'dynamic_bone_anomaly.png'\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Confidence Scores:\")\n",
    "for task, result in predictions.items():\n",
    "    conf = result['prob'].max()\n",
    "    print(f\"  {task.capitalize()}: {conf:.3f}\")\n",
    "print(\"===============================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
